---
title: 부캠 5주차 후기
layout: post
draft: true
use_math: true
---
### TODO

- 블로그 intro
- 블로그 Works 탭(포폴)
- 공부의 족적 정리(PCA 내용 정리하기)→ 차원감소법 정리하기
- latex 문법 정리하기
- ~~**Diffusion Note**~~
- [GNN 소개](https://distill.pub/2021/gnn-intro/)
- [GNN의 convolution에 대해](https://distill.pub/2021/understanding-gnns/)
- https://www.assemblyai.com/blog/an-introduction-to-poisson-flow-generative-models/
- https://www.assemblyai.com/blog/recent-developments-in-generative-ai-for-audio/
- [GraphSage](https://arxiv.org/abs/1706.02216?ref=assemblyai.com)
- Mamba
- [VLM을 소개하는 페이퍼](https://huggingface.co/papers/2408.12637)

## 토요일

[LG 채용 링크 정리](https://www.notion.so/LG-AI-76317cca7763409ca99b3c1ed49f2b7d?pvs=21)

~~코테 1문제 풀기~~

### Diffusion 마무리

#### GLIDE(Guided Language to Image Diffusion for generation and Editing)

- GLIDE with CLIP guidance
    - pretrained CLIP(Contrastive Language-Image Pre-training model)
    - Since $\log p(C\|X)\approx\frac{1}{\tau}f_\theta(X)\cdot g_\phi(C)+\text{constant independent of }X$, we can make use of this term as guiding score function.
    - But the problem is that $\log p(C\|X)\neq\log p(C\|X_t)$. So we pre-train time-dependent CLIP model.(trained separately from score network)
- GLIDE with Text-conditioning and classifier free guidance
    - In this case, we use classifier-free guidance, and uses a conditional error $\epsilon_\theta(X_t, t, C)$.
    - Here caption $C$ is embedded into a sequence of $K$ tokens and added to time embedding.
    - $K$ tokens are projected into key and value vectors of the attention layers of the U-Net, making cross-attention.

#### DALL-E 2

- **DALL-E 2**는 Image Encoder $f_\theta$, Text encoder $g_\phi$, Image decoder $h_\psi$, Prior $p_\omega$로 이루어진 모델이다.
- **Stage 1**: CLIP encoders $f_\theta$ and $g_\phi$: 다른 네트워크 학습 중엔 얼려짐
    
![dall-e2](https://github.com/user-attachments/assets/c68238b4-ca66-4854-afcf-e764b948b99b)
    
- Decoder $h_\psi$는 conditional diffusion model로 학습되어 sample을 생성한다. 다음 3가지 경우가 있다.
    - $h_\psi(f_\theta(X), \emptyset)\approx X$는 의미론적 의미를 추출할 때 사용되어 $p(\cdot\|f_\theta(X))$에서 샘플을 만든다. ‘bipartite 표현식’에 멋지게 응용가능
    - $h_\psi(f_\theta(X), C)\approx X$는 $C$가 $X$를 잘 묘사한다면 더 정확한데, 최종 텍스트-to-이미지 생성 과정에 쓰인다. $p(\cdot\|f_\theta(X), C)$에서 추출됨
    - $h_\psi(0, \emptyset)\approx X$는 자막 $C$에 대응되는 이미지를 만드는데, 그리 잘 작동하지는 않지만 classifier-free guidance에서 쓰인다. $p(\cdot\|C)$에서 추출됨
- **Stage 2**: 다음 조건부 에러 네트워크를 학습한다.

\\(\epsilon_\psi(X_t, t, Z^\text{image}, C)\\)

- 이 때, $X_0=0, Z^\text{image}=f_\theta(X)$이고 $C$는 이미지 자막 순서쌍인 $(X,C)$로 주어진다.
- 10% 확률로 $Z^\text{image}=0$인 경우와 50%확률로 $C=\emptyset$인 경우를 학습한다.
- 이걸로 64x64 이미지를 만들고, CDM으로 해상도를 올린다.
- **Bipartite representation**: Image decoder에서 caption을 비우고, DDIM sampler로 $(X_T, Z^\text{image}$를 만든다. 뭔가 이상한 노이즈 같겠지만..DDIM을 뒤로 돌리면 다시 원래 이미지를 복구할 수 있다. 이 $(X_T, Z^\text{image})$ 페어를 bipartite representation이라 한다.
    - 응용 1-변형: X로 bipartite representation을 구하면, 이를 통해 $(X_0, Z^\text{image})$를 샘플링하여 $X$의 다양한 변형을 얻을 수 있다. 이미지 인코더와 캡션이 빈 이미지 디코더 사용
    - 응용 2-보간: 두 이미지 $X^{(1)}, X^{(2)}$에 대해, $Z=\eta f_\theta(X^{(1)})+(1-\eta)f_\theta(X^{(2)})$를 구해서 DDIM 샘플러를 돌리면 $\eta$에 따라 다양한 샘플을 얻을 수 있다.(단, $X_T\sim\mathcal{N}(0, I)$는 고정). 이미지 인코더와 빈캡션의 이미지 디코더 사용
    - 응용 3-텍스트로 변화주기:$(X, C, C^\text{new})$와 bipartite representation이 주어졌을 때, $Z=f_\theta(X)+\eta(g_\phi(C^\text{new})-g_\phi(C))$를 구성해서 DDIM 샘플러를 때리면 캡션 1에서 캡션 2의 형태로 변화하는 이미지 구성을 볼 수 있다. 이미지 인코더, 디코더, 텍스트 인코더를 전부 사용.
- prior 없이 텍스트를 이미지로 바꾸는 작업을 해보면 문제가 생긴다.
    1. $h_\phi(0, C)$로 생성하기: 결과가 구림
    2. $h_\phi(g_\phi(C), C)$로 생성하기: $h$는 이미지 임베딩으로 학습됐는데 강제로 텍스트 임베딩 $g$를 쓴 게 문제긴 하다. 결과는 좀 더 낫긴 하다
    3. prior $p_\omega$ 사용하기→최고의 결과를 얻음
- Prior $p_\omega$는 $p(Z^\text{image}\|Z^\text{text}, C)$로부터 샘플을 만든다. 수학적으로는 $Z^\text{text}=g_\phi(C)$라 필요 없는 조건이고 $p(X\|C)$로 충분해보이지만, 실전에선 CLIP-pretrained-feature인 $g_\phi(C)$가 필수인 것으로 드러났다.
- Prior를 어떻게 학습할 것인가?
    - auto-regressive model은 잘 작동하지 않는다.
    - Diffusion에 기반하여 학습시킨다. 이 때 순수 transformer 모델을 사용한다. 애초에 latent variable은 이미지가 아니라서 conv. layer가 도움이 안되기 때문.(실제로 그럼)
- 최종 DALL-E 2 생성 프로세스는 다음과 같다
    1. $Z^\text{text}=g_\phi(C)$ 계산
    2. latent variable인 $Z^\text{image}=p_\omega(Z^\text{text}, C)$ 생성
    3. 이걸로 원하는 이미지 $X$를 $h_\phi(Z^\text{image}, C)$에서 생성

#### Imagen

- **Imagen**은 단순히 CDM인데, 사전 학습된 LLM으로 text imbedding을 함.
- Imagen 학습 프로세스
    1. T5(Text-To-Text Transfer Transformer)같은 LLM을 사전 학습시키고 얼림(이미지 안씀)
    2. 이미지-캡션 페어로 CDM 학습
    3. ‘Dynamic thresholding’ 기법을 통해 classifier-free guidance로 이미지 생성
- Classifier-free guidance에선 보통 큰 스케일 파라미터가 필요한데, 이러면 이미지 일치율이 구려지는 문제가 있음. 이미지가 포화되기 때문
- Dynamic thresholding을 간단히 설명하면, 이미지 픽셀 값을 잘라내지 않고 천천히 적정범위로 밀어넣는 테크닉임.
- 흥미롭게도, 에러 네크워크를 키우는 것보다 텍스트 인코더(여기서 LLM)을 키우는게 훨 중요하단 결과가 나옴

#### Latent Diffusion Model

- 현재 Diffusion 모델의 문제점: 이미지에 직접 작용하기 때문에
    - 전체 이미지에 1000번 이상 스텝을 진행하는게 비효율적이고
    - 적용 범위가 좁아진다. 텍스트엔 어캐 할건데?
- 여기서 나온게 Variational autoencoder에 diffusion을 하는 것
![latent_diffusion](https://github.com/user-attachments/assets/85ba6d68-b172-4bc9-9525-cc104c2ecf7d)

- VLB 항을 3개로 쪼갤 수 있다: 순서대로 reconstruction term, negative encoder entropy, cross-entorpy

\\(
\text{VLB}\_{\phi, \theta, \psi}(X)=\mathbb{E}_{Z_0\sim q_\phi(\cdot\|X)} [-\log p_\psi(X\|Z_0)]+D_\text{KL}(q_\phi(\cdot\|X)\||p_\theta(\cdot))\\\ 
=\mathbb{E}\_{Z\_0 \sim q\_\phi(\cdot\|X)} [-\log p_\psi(X\|Z_0)]+\mathbb{E}_{Z_0\sim q_\phi(\cdot\|X)} [\log q_\phi(Z_0\|X)]+\mathbb{E}\_{Z_0\sim q_\phi(\cdot\|X)} [-\log p_\theta(Z_0)]
\\)

- 일반적인 VAE에선 $q_\phi(\cdot\|X)=\mathcal{N}(\mu_\phi(X), \Sigma_\phi(X))$와 $p_\psi(\cdot\|Z_0)=\mathcal{N}(f_\psi(Z), \sigma^2 I)$니까, 앞의 두 항은 샘플링과 재매개변수화 트릭, 백프롭 등으로 쉽게 계산 가능하다.
- 마지막 항은 score matching으로 해결한다.

\\(
\text{CE}(q_\phi(\cdot\|X)\||p_\theta(\cdot)=\underset{t~\mathcal{U}[0,1]}{\mathbb{E}}\left[\frac{w(t)}{2}\underset{\substack{Z_0\sim q_\phi(\cdot\|X)\\\ \epsilon\sim\mathcal{N}(0, T)\\\ Z_t=\mu_t(Z_0)+\sigma_t\epsilon}}{\mathbb{E}}[\||\epsilon-\epsilon_\theta(Z_t, t)\||^2]\right]+\frac{d_Z}{2}\log(2\pi e \sigma_0)\\)

where $\mu_t(Z_0)$ is the mean of $Z_t$ conditioned on $Z_0$ under the SDE $dZ_t=f(t)Z_t dt+g(t)dW_t$

- Latent diffusion model의 최종 학습 과정:
    1. prior $p_z=\mathcal{N}(0,I)$ 하에서 VAE $(q_\phi, p_\psi)$를 사전학습
    2. 처음부터 끝까지 diffusion model과 VAE 전체 $(q_\phi, p_\psi, p_\theta)$를 학습시킨다. VAE를 고정시켜도 되지만, 같이 학습하는게 결과가 더 좋더라
- VAE가 가우시안 prior에서 사전학습되므로 디퓨전의 최종 마지널 분포도 가우시안이다. 이 상태에서 SDE를 VP-SDE가 되도록 고르면,  $p_\theta$의 학습이 일반적인 경우보다 훨씬 쉬워진다.
- 또한 일반적인 Diffusion에선 $X_0,X_1$이 엄청 다르지만, latent diffusion에선 $Z_0,Z_1$의 분포가 거의 비슷하다.

#### Stable Diffusion

- **Stable Diffusion**: Autoencoder를 사전학습시키고 얼린 다음 Latent Diffusion model을 사용한 것. 그 이후 latent variable에 대한 conditional diffusion model을 만든다.
- 이후 수많은 혁신을 낳게 된다.
