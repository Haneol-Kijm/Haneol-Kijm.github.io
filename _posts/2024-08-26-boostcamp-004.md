---
title: 부스트캠프 4주차 후기
layout: post
draft: true
use_math: true
---

### TODO

- 블로그 intro
- 블로그 Works 탭(포폴)
- 공부의 족적 정리(PCA 내용 정리하기)→ 차원감소법 정리하기
- latex 문법 정리하기
- **Diffusion Note**
- [GNN 소개](https://distill.pub/2021/gnn-intro/)
- [GNN의 convolution에 대해](https://distill.pub/2021/understanding-gnns/)
- https://www.assemblyai.com/blog/an-introduction-to-poisson-flow-generative-models/
- https://www.assemblyai.com/blog/recent-developments-in-generative-ai-for-audio/
- [GraphSage](https://arxiv.org/abs/1706.02216?ref=assemblyai.com)
- Mamba

## 토요일

- 드디어 github pages에서 mathjax를 이용해서 완전히 latex 수식을 구현하는 것에 성공
    - $$ 기호를 못알아먹어서 \\( 기호로 표기함
    - \newline 명령어도 안 들어먹던데 \*3번 쓰는 걸로 라인브레이크를 만듦
    - 행렬에서도 똑같이 triple slash로 되긴 하는데, 엔터를 치면 안됨. 하..ㅋㅋ;
    - 또 다른 이슈…[]그냥 브래킷을 씌우면 인식이 안되서 \[\] 식으로 꼭 써줘야함
    - 마크다운의 더블 언더바(__)도 이탤릭 처리와 헷갈려서, 언더바가 많이 등장하는 경우에 \_ 식으로 처리해줘야 알아먹음. $$ 기호를 \\( 기호로 바꾼 것의 스노우볼 같은데, 수식 인식이 아니라 마크다운 인식을 거친 후에 수식으로 바뀌는 식이라 마크다운 양식과 충돌이 너무 많다
- CS231A 과제1을 주로 하는데 수업이 수업이라 그런지 난이도가 상당한 느낌. 다행히 2,3번은 아직 배우지 않은 내용이라 스킵하고 과제0 파이썬과 선대 연습 쪽 과제를 해야겠다.

## 일요일

- CS231A 과제0 완료. ndarray.shape 호출할 때 pair가 호출되므로 shape[0] 식으로 불러야 함에 주의(소괄호인줄 알았음;)
- 또한 [np.dot](http://np.dot) 사용할 때, 모양을 직접 맞춰서 곱해줘야 하므로 np.dot(a.T, b) 형식으로 곱해줘야함.
- (3,) 꼴의 벡터를 (3,1)로 만드려고 np.expand_dims(a, axis=-1)을 쓰긴 했는데…다른 좋은 방법이 없을까?

Diffusion ch.3

- Score network의 구성요소
    - [GELU](https://paperswithcode.com/method/gelu), [SiLU(swish)](https://paperswithcode.com/method/silu)
    - U-Net
    - Time embedding: trasnformer의 positional encoding과 유사하게
    - residual block: 1x1 conv를 지나는 residual connection이 있으며, time embedding은 scale-shift block에 scale과 shift로써 사용된다.
    - Pixel-wise multi-head self-attention:
        - single pixel with multi channel에 적용
        - ViT처럼 patch-wise로 적용하는 것이 아니며, conv layer와 섞어쓴다.
    - GroupNorm
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/a55af6f1-b18a-4142-b142-46acd8e866c3/image.png)
        
- Score network structure
    - 내려가는 블록: downsampling, res block, res block, attn block
    - 올라가는 블록: concat, res block, res block, attn block, upsampling
    - 각 res block은 time embedding 값을 받아서 씀
- Tweedie’s formula: [읽을 것](https://alexxthiery.github.io/posts/reverse_and_tweedie/reverse_and_tweedie.html)
- Reverse conditional dist. $\approx$ Gaussian
    - Q. Why $p_X(x)=p_X(y)+\langle\nabla p_X(y), x-y\rangle +O(\|x-y\|^2)$?
    - Reverse conditional distribution이 Gaussian인 것을 유도하는 과정 자체에서도 reverse diffusion process의 additional drift term이 score fucntion이 됨을 [유도할 수 있다](https://alexxthiery.github.io/posts/reverse_and_tweedie/reverse_and_tweedie.html).
    - [Reverse Diffusion Process](https://www.notion.so/Reverse-diffusion-process-0cd41bccac244aacab4e73c496c817bc?pvs=21)

## 16일차

### 데일리 트렌드

- [LLM의 원리를 시각화 한 영상](https://x.com/Hamptonism/status/1827439848426500303). 고차원에 임베딩된 단어 벡터들은 방향 그 자체로 의미를 형상화한다는 내용이 있는데, 일리있고 신기하단 생각이 든다.
- LLM의 컨트롤을 아예 [서베이 페이퍼](https://huggingface.co/papers/2408.12599)로. 이 컨트롤 작업도 태스크가 내용 컨트롤과 특성 컨트롤 2가지로 나뉘는 모양이다. 시간만 된다면 꼭 읽어보고 싶은 논문
- [휴먼 비전 파운데이션 모델](https://huggingface.co/papers/2408.12569). 무슨 모델을 쓰나 봤더니 [Masked Autoencoder](https://arxiv.org/pdf/2111.06377)라는 생소한 모델을 쓰고 있었다.
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/59c5d197-f7e6-4fa7-a306-47e3266cfbe8/image.png)
    
    - 패치단위로 학습하는것까진 ViT, MLPMixer와 유사하다.
    - 그러나 빈 칸을 채우는 식으로 학습한다.
    - 이런 식으로 학습한 모델은 일반화도 잘되며, 다른 다운스트림 태스크도 잘 수행한다고 한다.

## 부캠 강의

- 극좌표계를 matplotlib으로 그려보는데, rmin을 세팅해도 도넛 모양이 되지는 않고 원 중심값이 min값이 되버린다…도넛은 matplotlib으로 그리기 번거로울 걸로 예상이 된다. 아마 동심원을 써서 도넛인 첫 해야할 듯.
- 역시 도넛은 예상대로 pie chart에 빈 원.

## 개인학습

Diffusion note Ch.3

- 왜 DDPM의 분산은 ‘1-각 분산을 1에서 뺀 후 다 곱한것’ 이 되는 걸까
- DDPM loss 유도가 이해가 안된다. 원본 논문을 읽어봐야할듯
- DDPM에 나오는 모든 분산 값들의 유도 과정이 이해가 제대로 되지 않는다. 대략 $X_t=\sqrt{1-\sigma}X_{t-1}+\sigma Z$ 꼴 일때 계산이 되는 것 같긴 한데, 다음에 이 내용을 다시 공부한다면 이해할 수 있을까?
- DDPM은 forward 건 sampling이건 VP SDE의 이산화된 과정이 맞다. 어찌보면 DDPM에서 설정한 평균과 분산이 이를 노린 것으로 보이기도 한다.
- DDPM loss는 variational lower bound(VLB)를 통해서도 구할 수 있다.
- DDIM은 marginal distribution이 DDPM과 같아지도록 만든 non-makov process
- 따라서 학습 과정이 같다. sampling만 다르다
- $\rho_t$ 변수가 있는데, 0으로 설정하면 deterministic sampling이 가능하다.
- DDIM은 VP ODE의 이산화 과정이다. marginal이 같으므로 foward case는 DDPM과 증명이 동일. Sampling도 deterministic DDIM sampling은 Euler discretization of VP ODE이다.
- 어찌저찌 이해는 하겠다만 결국 conditional distribution의 분산을 유도하는 계산식이 이해가 잘 가지 않는다. 왜 $X_t\|X_{t-1}\sim\mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I)$일 때 $X_t\|X_0$의 분산이 $(1-\prod_s (1-\beta_s))I$인 것인지.
