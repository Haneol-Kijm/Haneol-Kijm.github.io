---
title: 부스트캠프 4주차 후기
layout: post
draft: true
use_math: true
---
### TODO

- 블로그 intro
- 블로그 Works 탭(포폴)
- 공부의 족적 정리(PCA 내용 정리하기)→ 차원감소법 정리하기
- latex 문법 정리하기
- **Diffusion Note**
- [GNN 소개](https://distill.pub/2021/gnn-intro/)
- [GNN의 convolution에 대해](https://distill.pub/2021/understanding-gnns/)
- https://www.assemblyai.com/blog/an-introduction-to-poisson-flow-generative-models/
- https://www.assemblyai.com/blog/recent-developments-in-generative-ai-for-audio/
- [GraphSage](https://arxiv.org/abs/1706.02216?ref=assemblyai.com)
- Mamba

## 토요일

- 드디어 github pages에서 mathjax를 이용해서 완전히 latex 수식을 구현하는 것에 성공
    - $$ 기호를 못알아먹어서 \\( 기호로 표기함
    - \newline 명령어도 안 들어먹던데 \*3번 쓰는 걸로 라인브레이크를 만듦
    - 행렬에서도 똑같이 triple slash로 되긴 하는데, 엔터를 치면 안됨. 하..ㅋㅋ;
    - 또 다른 이슈…[]그냥 브래킷을 씌우면 인식이 안되서 \[\] 식으로 꼭 써줘야함
    - 마크다운의 더블 언더바(__)도 이탤릭 처리와 헷갈려서, 언더바가 많이 등장하는 경우에 \_ 식으로 처리해줘야 알아먹음. $$ 기호를 \\( 기호로 바꾼 것의 스노우볼 같은데, 수식 인식이 아니라 마크다운 인식을 거친 후에 수식으로 바뀌는 식이라 마크다운 양식과 충돌이 너무 많다
- CS231A 과제1을 주로 하는데 수업이 수업이라 그런지 난이도가 상당한 느낌. 다행히 2,3번은 아직 배우지 않은 내용이라 스킵하고 과제0 파이썬과 선대 연습 쪽 과제를 해야겠다.

## 일요일

- CS231A 과제0 완료. ndarray.shape 호출할 때 pair가 호출되므로 shape[0] 식으로 불러야 함에 주의(소괄호인줄 알았음;)
- 또한 [np.dot](http://np.dot) 사용할 때, 모양을 직접 맞춰서 곱해줘야 하므로 np.dot(a.T, b) 형식으로 곱해줘야함.
- (3,) 꼴의 벡터를 (3,1)로 만드려고 np.expand_dims(a, axis=-1)을 쓰긴 했는데…다른 좋은 방법이 없을까?

Diffusion ch.3

- Score network의 구성요소
    - [GELU](https://paperswithcode.com/method/gelu), [SiLU(swish)](https://paperswithcode.com/method/silu)
    - U-Net
    - Time embedding: trasnformer의 positional encoding과 유사하게
    - residual block: 1x1 conv를 지나는 residual connection이 있으며, time embedding은 scale-shift block에 scale과 shift로써 사용된다.
    - Pixel-wise multi-head self-attention:
        - single pixel with multi channel에 적용
        - ViT처럼 patch-wise로 적용하는 것이 아니며, conv layer와 섞어쓴다.
    - GroupNorm
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/a55af6f1-b18a-4142-b142-46acd8e866c3/image.png)
        
- Score network structure
    - 내려가는 블록: downsampling, res block, res block, attn block
    - 올라가는 블록: concat, res block, res block, attn block, upsampling
    - 각 res block은 time embedding 값을 받아서 씀
- Tweedie’s formula: [읽을 것](https://alexxthiery.github.io/posts/reverse_and_tweedie/reverse_and_tweedie.html)
- Reverse conditional dist. $\approx$ Gaussian
    - Q. Why $p_X(x)=p_X(y)+\langle\nabla p_X(y), x-y\rangle +O(\|x-y\|^2)$?
    - Reverse conditional distribution이 Gaussian인 것을 유도하는 과정 자체에서도 reverse diffusion process의 additional drift term이 score fucntion이 됨을 [유도할 수 있다](https://alexxthiery.github.io/posts/reverse_and_tweedie/reverse_and_tweedie.html).
    - [Reverse Diffusion Process](https://www.notion.so/Reverse-diffusion-process-0cd41bccac244aacab4e73c496c817bc?pvs=21)

## 16일차

### 데일리 트렌드

- [LLM의 원리를 시각화 한 영상](https://x.com/Hamptonism/status/1827439848426500303). 고차원에 임베딩된 단어 벡터들은 방향 그 자체로 의미를 형상화한다는 내용이 있는데, 일리있고 신기하단 생각이 든다.
- LLM의 컨트롤을 아예 [서베이 페이퍼](https://huggingface.co/papers/2408.12599)로. 이 컨트롤 작업도 태스크가 내용 컨트롤과 특성 컨트롤 2가지로 나뉘는 모양이다. 시간만 된다면 꼭 읽어보고 싶은 논문
- [휴먼 비전 파운데이션 모델](https://huggingface.co/papers/2408.12569). 무슨 모델을 쓰나 봤더니 [Masked Autoencoder](https://arxiv.org/pdf/2111.06377)라는 생소한 모델을 쓰고 있었다.
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/59c5d197-f7e6-4fa7-a306-47e3266cfbe8/image.png)
    
    - 패치단위로 학습하는것까진 ViT, MLPMixer와 유사하다.
    - 그러나 빈 칸을 채우는 식으로 학습한다.
    - 이런 식으로 학습한 모델은 일반화도 잘되며, 다른 다운스트림 태스크도 잘 수행한다고 한다.

## 부캠 강의

- 극좌표계를 matplotlib으로 그려보는데, rmin을 세팅해도 도넛 모양이 되지는 않고 원 중심값이 min값이 되버린다…도넛은 matplotlib으로 그리기 번거로울 걸로 예상이 된다. 아마 동심원을 써서 도넛인 첫 해야할 듯.
- 역시 도넛은 예상대로 pie chart에 빈 원.

## 개인학습

Diffusion note Ch.3

- 왜 DDPM의 분산은 ‘1-각 분산을 1에서 뺀 후 다 곱한것’ 이 되는 걸까
- DDPM loss 유도가 이해가 안된다. 원본 논문을 읽어봐야할듯
- DDPM에 나오는 모든 분산 값들의 유도 과정이 이해가 제대로 되지 않는다. 대략 $X_t=\sqrt{1-\sigma}X_{t-1}+\sigma Z$ 꼴 일때 계산이 되는 것 같긴 한데, 다음에 이 내용을 다시 공부한다면 이해할 수 있을까?
- DDPM은 forward 건 sampling이건 VP SDE의 이산화된 과정이 맞다. 어찌보면 DDPM에서 설정한 평균과 분산이 이를 노린 것으로 보이기도 한다.
- DDPM loss는 variational lower bound(VLB)를 통해서도 구할 수 있다.
- DDIM은 marginal distribution이 DDPM과 같아지도록 만든 non-makov process
- 따라서 학습 과정이 같다. sampling만 다르다
- $\rho_t$ 변수가 있는데, 0으로 설정하면 deterministic sampling이 가능하다.
- DDIM은 VP ODE의 이산화 과정이다. marginal이 같으므로 foward case는 DDPM과 증명이 동일. Sampling도 deterministic DDIM sampling은 Euler discretization of VP ODE이다.
- 어찌저찌 이해는 하겠다만 결국 conditional distribution의 분산을 유도하는 계산식이 이해가 잘 가지 않는다. 왜 $X_t|X_{t-1}\sim\mathsc{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I)$일 때 $X_t|X_0$의 분산이 $(1-\prod_s (1-\beta_s))I$인 것인지.

## 17일차

### 데일리 트렌드

- [VLM을 소개하는 페이퍼](https://huggingface.co/papers/2408.12637). 시간 날 때 읽어보고 싶다. 이번 수업에서도 다룬다고 하니 주말 이용해서 읽으면 좋아 보인다.

### Github 강의

### 개인 학습

CS231A

- Magnification이 줄면 barrel 현상, 늘면 pincushion 현상. 근데 **magnification이 뭐지?**
- [lecture 3 후기](https://haneol-kijm.github.io/2024/08/27/CS231A-003)

Diffusion ch.4

- MCMC는 따로 공부한 적이 있어서 이해할 수 있다. 친근한 분포의 sampling을 통해 적분하기 어려운 식의 적분값을 sample에 함숫값을 먹인 것의 평균으로 추정하는 것이다.
- Langevin MCMC라는 것은 뭘까. MCMC에 대한 설명을 적고 나니 이해할 것 같다.
- 일단 forward-reverse의 과정은 아니고, score fucntion+Wiener process term을 갖는 random process를 만들면 원래 distribution으로 수렴하게 된다.
$dX_t=\frac{1}{2}\nabla\log p(X_t)dt+dW_t$
- 이 때 score function을 학습시킨 뒤 MCMC 마냥 sampling해서 최종적으로는 데이터의 분포에서 sampling을 하고자하는 것이 목적인 것이다.
- 문제 1: [Support of distribution](https://en.wikipedia.org/wiki/Support_(mathematics))=확률이 0이 되는 가장 큰 열린 집합의 여집합. 즉 data distribution이 full suport가 안되서 꽉 채우지 못한다면(좀 차원이 낮다던지), 서포트 바깥에서의 $\log P(X)$ 값이 마이너스 무한대가 되서 계산이나 학습 자체가 불가능함. 즉 애초에 SSM을 적용해서 score function을 학습시킬 수 없음
- 그럼 로그값이 0이 되지 않도록 좀 perturb한 분포에 대해 DSM이나 SSM을 적용한다면? 그것도 안됨. 왜? 그건 문제 3에서
- 문제 2: 랑제빈 MCMC의 또다른 문제점으로는 수렴이 보장되지만 속도가 너무 느리다는 점이 있음. 따라서 급발진하는 **‘온도’ $\tau$**를 설정해준 후, 이 온도를 점점 낮춰줌
- Annealed langevin(천천히 식는 랑제빈): $dX_t=\frac{1}{2\tau}\nabla\log p(X_t)dt+dW_t$
    
    각 온도에서 sampling 이후, 마지막 sample을 다음 온도의 첫 sample로 사용
    
- 문제 3: 역시나 여전히 확률이 낮은 지점에선 score function 학습이 잘 안됨. 하지만 annealing 하는 과정에서 높은 온도에 있는 분포가 이 확률이 낮은 지점들을 찍고 다니는 경우가 생겨버림. 아니면 multimodal인 경우에 피크를 옮기는 과정에서 생기는 확률 낮은 점들을 찍는다던지
- 결국 해결책은? 아까 perturb noise를 크게 시작해서 점점 줄여나가고, 모든 noise에 대해 score fucntion을 학습시킨다→**Noise Conditioned Score Network(NCSN)**
- NCSN에선 $X~p_{\text{data}}$고 $\epsilon$이 정규분포인 경우에 다음과 같이 정의한다

\\(

X+\sigma\epsilon\sim\tilde{p}^\sigma_{\text{data}}\\)

이 이후 학습은 DSM으로 하고 온도 annealing과 유사하게 진행

## 18일차

### 데일리 트렌드

- [Pytorch는 죽었다 JAX 최고](https://news.hada.io/topic?id=16369):
- [실용적 코드 편집기](https://news.hada.io/topic?id=16400&utm_source=weekly&utm_medium=email&utm_campaign=202435)
- [인간 vs 언어모델](https://news.hada.io/topic?id=16368&utm_source=weekly&utm_medium=email&utm_campaign=202435)
