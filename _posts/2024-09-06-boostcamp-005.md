---
title: 부캠 5주차 후기
layout: post
draft: true
use_math: true
---
<details>

### <summary>TODO</summary>

<div markdown=”1”>

- 블로그 intro
- 블로그 Works 탭(포폴)
- 공부의 족적 정리(PCA 내용 정리하기)→ 차원감소법 정리하기
- latex 문법 정리하기
- **~~Diffusion Note~~**
- [GNN 소개](https://distill.pub/2021/gnn-intro/)
- [GNN의 convolution에 대해](https://distill.pub/2021/understanding-gnns/)
- https://www.assemblyai.com/blog/an-introduction-to-poisson-flow-generative-models/
- https://www.assemblyai.com/blog/recent-developments-in-generative-ai-for-audio/
- [GraphSage](https://arxiv.org/abs/1706.02216?ref=assemblyai.com)
- Mamba+[LinFusion](https://huggingface.co/papers/2409.02097)
- [~~VLM을 소개하는 페이퍼~~](https://huggingface.co/papers/2408.12637)
- [MAE 논문](https://arxiv.org/abs/2111.06377)

</div>

<details>

## 토요일

[LG 채용 링크 정리](https://www.notion.so/LG-AI-76317cca7763409ca99b3c1ed49f2b7d?pvs=21)

~~코테 1문제 풀기~~

[diffusion 내용 정리](https://haneol-kijm.github.io/2024/09/01/diffusion-001)에서 ch.5 부분 전부 공부함

## 일요일

### VLM paper 읽기

1. [VLM from diffusion note](https://ernestryu.com/courses/FM.html)
CLIP에 대해 소개하고 CLIP에서 쓰이는 $\mathcal{L}_\text{NCE}$가 mutual information에 의한 upper bound를 가지며, 이를 근사할 수 있는 조건이 사실 해석학 시간에 배우는 Stone-Weistrauss나 Hilbert space의 density로 설명할 수 있음을 얘기해주어 흥미로움을 돋군다.
2. [**Building and better understanding vision-language models: insights and future directions**](https://huggingface.co/papers/2408.12637)
- VLM은 핫한 연구분야지만 무엇이 더 좋은지에 대한 합의가 부족하다.
- 이 논문에서는,
    - 최근 VLM에서 발생하는 문제 소개
    - 최신 모델들이 이를 대처하는 방법들 소개하고 장단점 설명
    - 사전 학습된 언어모델과 비전 인코더를 엮는 다양한 방식
    - VLM에서 쓰이는 다양한 데이터 형식과 활용도
    - VLM의 여러 단계의 학습방식
    - 모델 평가에 있어서의 난점
    - 미래 연구 지향점
    - 그 이후 이것들을 활용해서 직접 모델을 훈련시켜보겠다

#### VLM의 모델 선택 문제

##### 사전 학습된 unimodal model을 쓰게 된 계기

Vision encoder든 LLM이든 학습이 비싸서, open source로 이게 열리니까 많은 연구자들이 두 개를 엮어서 쓰게 되었다. 엮는 방식은 cross-attention과 self-attention으로 갈린다.

- Cross Attention: 랭귀지 모델을 얼린 뒤, image hidden state가 conditioning으로 들어간다. 여기서 사전 학습된 랭귀지 모델 레이어 사이에 초기화된 크로스-어텐션 레이어가 엮여들어간다. 이 레이어의 파라미터 사이즈는 LLM의 4분의 1가량으로 표현을 풍부하게 함
- Self Attention: 여기선 vision encoder 결과물이 token이 되어, text token에 붙여지고 언어모델 인풋으로 쓰인다. 최근 VLM들이 이 형식을 따른다.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/be0bcbdf-d974-4069-bf51-30ca529cd168/image.png)

- 뭐가 더 낫냐? Cross attention이 얼린 상태에서는 엄청 좋다가 안 얼리면 확 구려지는 실험이 있기는 한데, 제대로 된 조건에서 비교한 내용이 없음
- 사실 VLM의 성능이 backbone model 성능을 크게 따라간다는 연구가 많음. 그래서 어떤 모델은 아예 효율을 포기하고 인코더를 여러 개 쓰는 놈도 있음
- 현재는 그래서 open 언어 모델이 많음. 근데 vision encoder 공개된게 좀 부족함.

##### 다른 관점에서 VLM 구조 선택을 몇 가지 바라보기

- Vision Encoder는 필수인가??
    - 어떤 놈은 이미지 패치를 직빵으로 LLM에 때려박기도 했음. 이러면 다른 모델에 대한 의존도도 없고, 이미지 정보를 온전히 쓸 수 있음
    - 어떤 사람은 유저 명령이 이미지 인코더 아웃풋과 독립적인게 싫어서, 인코더에 유저 명령을 집어넣기도 함.
    - 근데 이런 모델들이 특출나게 뛰어난 결과는 못 보여줌.
- 그럼 비전 인코더랑 언어모델을 어떻게 엮지?
    - linear layer 하나로 퉁치는 경우도 있었는데 학습과 인퍼런스 효율이 떨어짐
    - 크로스 어텐션 1개를 쓰거나, perceiver resampler 내부에 쓰기도 함
    - visual token은 최대한 줄이면 64개까지 줄일 수 있지만, OCR 태스크의 경우에는 갯수를 늘릴수록 좋다는게 알려져있다.
    - perceiver resampler가 계속 효율적인지는 다양한 논문에서 도전받고 있다.
- 이미지 스플리팅: 이미지 토큰 갯수 뻥튀기 트릭
    - 일반적인 비전 인코더들은 고해상도 이미지를 다루기 힘듦
    - 이미지 스플리팅 트릭을 통해, 사전 학습된 비전 인코더에게 여러 파편 이미지를 주는 것으로 이걸 해결
    - 단, 각 이미지 조각이 독립이 아니므로, 이걸 각각 인코딩하는건 글로벌 특성을 잃을 수 있단 우려가 있음. 그래서 다운스케일 저해상도 이미지를 같이 쓰기도 하지만, 완벽한 해답은 아님.
    - 그럼 해결책은? 비전 인코더가 앞으로 고해상도도 접근할 수 있도록 만들어야 함. 이 모델은 여러 이미지 토큰을 제공하고, 이미지를 쪼갤 필요 없이 한 방에 돌아갈 수 있어야 함.

**VLM의 학습방법과 데이터셋, VLM을 평가할때 생기는 문제점** 부분 도 논문에 남아있지만 시간 관계상 여기까지 정리하고 싶다.

### CS231A 스터디

- 스터디를 함으로서 모르고 넘어가는 부분에 대해서 서로 도움을 받고 알게되는 점들이 있다. projective transformation을 나는 단순히 collinearity를 보존하는 매핑이라고만 생각했는데, 팀원이 현실 사진의 관점을 옮기는 매핑으로 이해할 수 있다고 해서 생각해보니 좀 더 직관적으로 와닿게 되었다. 스터디에서 큰 도움을 받아가는 것 같다.
- 스터디 회의록을 쓴다거나, 다 같이 과제 풀이를 본다거나 하는 욕심이 있었지만, 오늘 해보니 1시간 안에 소화하기도 힘들고 다 따라잡기도 힘들어서 안 하기로 하였다. 때로는 원하는 목표를 위해 타협하는 것도 필요하단 생각이 든다.

### Toy-diffusion

- 예전에 부스트캠프를 들어오기 전 잠깐 시간 내서 했었던 toy diffusion model의 code를 읽어보았다.
- alpha와 beta 값의 구성을 보고 깨달은 것은, 여기 쓰인 모델이 정확이 DDPM(또는 DDIM)이라는 것이다.
- 안 맞는 수치들이 있어서, 제대로 이해해보려면 DDPM 원본 논문을 읽던가 해야할듯..
- latent diffusion model도 직접 손으로 구현해보던, 코드를 퍼오고 수정해서 toy project로 쓸 수 있으면 좋을 것 같단 생각이 든다. 직접 구현해봐야 어려움과 원리를 이해해볼 수 있지 않을까.

## 21일차

### 데일리 트렌드

- RAG

![1724837905245.gif](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/b87f5ba2-b660-46f2-9a2e-6af7d70fabe6/1724837905245.gif)

- 허깅페이스에 올라오는 [daily paper의 요약본을 자동화해서 모음집으로](https://x.com/gabrielchua_/status/1829726300963312123) 올려주는 깃헙 액션. 생각은 했지만 실제로 올라올 줄이야…앞으론 이쪽에서 보는 편이 훨씬 편할 것 같다.
- [Multimodal LLM에서 쓰는 비전 표현식의 연산량을 획기적으로 줄이는 방법](https://huggingface.co/papers/2408.16357). 99.7%의 연산량 감소라는 충격적인 수치를 보았다. 기억해두면 좋을듯?

### 부캠강의

- 3강: attention의 visualization을 접해본 적이 없었는데 보게 되어 신기했다. 계속 쓰이는 레이어인만큼 좀 더 구체적으로 알아보고 싶기도 하다. Grad-CAM도 알게 되어 좋았다.
- 4강: 짧은 시간 안에 많은 모델을 익히고 공부하기가 어려운데, 개념정리를 되게 깔끔하게 해주셔서 보기 좋다는 인상을 받았다. Task 별로 모델의 역사를 공부하는게 크게 도움이 되는 것 같다. 최신 정보까지 다루어주어 방향성을 명확하게 짚어주는 점도 좋았다.

<details>

<summary>4강-Segmentation and Detection</summary>

- Segmentation-sementic category로 분리하는 것, sementic segmentation
    
    모델-Fully convolutional network(FCN)-U-Net
    
- Object Detection-사물 박스로 탐지+classification
    
    모델-2 stage detector R-CNN-Fast R-CNN- Faster R-CNN-YOLO(1-stage detector)-RetinaNet
    
- Instance Segmentation-sementic segmentation+distinguishing instance
    
    모델-Mask R-CNN
    
- Transformer-based models
    
    모델-DETR(DEtection TRansformer)-MaskFormer(Sementic Transformer)-Uni-DVPS(Unified Model for Depth-Aware Video Panoptic Segmentation)
    
- Segmentation Foundation Model-SAM(Segment Anything Model)
    - Image encoder, Prompt encoder, Mask decoder로 구성 됨
    - 데이터셋 부족을 해결하기 위해 데이터로 모델을 만듦-그 모델로 데이터를 뽑아냄 이걸 반복
    - Grounding DINO 모델과 결합하여 open vocab.으로 classification하는 지점까지 현재 연구됨

</details>

## 개인 학습

[CS231A Lecture 6](https://haneol-kijm.github.io/2024/09/02/CS231A-006)

## 22일차

### 데일리 트렌드

- [VQ4DiT](https://x.com/_akhaliq/status/1830624683504304292): 에지 디바이스라는 제한된 리소스에서 Vector Quantization(VQ)란 기법을 통해 디퓨전 트랜스포머를 돌리는 방법

### 부캠 강의

<details>

<summary>5강-Computational Imaging</summary>

#### Computational Imaging이란?

- 원래 카메라에선 이미지를 저장하기까지 복잡한 프로세스를 거침. 계산사진학에선 이 과정에 계산을 더함. 본 과정에선 복구랑 향상만
- 딥러닝으로 하는 방법도 있음. U-Net 많이 씀

#### Training Data in Computational Imaging

- 노이즈 제거: 요즘은 다 디노이즈 달고 나오지만, 포톤노이즈, 리드노이즈, 퀀티제이션노이즈 등이 있음. 노이즈는 가우시안 노이즈로 쉽게 모델링해서 합성 노이즈 사진도 만들 수 있음.
- 고해상도: 저해상도 이미지를 고해상도로 복구하는 작업. 다운 샘플링을 통해 합성저해상도를 만들 수 있음. 좀 더 실질적인 학습 데이터 페어를 위해, RealSR에선 ‘얇은렌즈’ 모델을 사용해서 실제 고해상도 이미지+먼 거리에서 찍은 이미지를 확대한 저해상도 이미지 식으로 이미지를 만들어냄. 그 이후 피라미드 구조로 고해상도를 학습
- 뿌얘짐 제거: 카메라 흔들림이나 사물 움직임으로 발생하는게 뿌연 이미지. 간단한 합성으로 만들 수 있긴 한데 현실 블러랑 거리가 있음. 그래서 실제 블러 데이터 페어를 만들어서 학습하는 연구들이 진행됨.
    - GoPro에선 고프레임으로 쪼개서 촬영한 것을 합치는 식. 그러나 원하지 않는 지점도 불연속한 블러가 생기는 문제가 있음
    - RealBlur에선 빠른 카메라와 느린 카메라, 광선 쪼개기 거울을 사용하여 블러 이미지를 만듦
- 비디오 모션 강화: 얘는 위 문제들과 다르게 실제 이미지와 강화 이미지 페어가 없음. 합성을 써야만 함.
    - 혼합과 평행이동으로 만드는 경우
    - CNN과 2개의 프레임을 통해 간단하게 합성하는 방법도 제안됨
- 딥러닝 이미지 복구: 위에서 나온 모든걸 할 때 노이즈 레벨을 고정한다던가 하면 문제가 생김. On-demand learning이란 게 제시됨. 다양한 레벨의 파라미터를 고려해야된다는게 중요함

#### Advanced Loss functions

- L2와 L1 로스를 많이 쓰지만, 별로 정확하지 않음(노이지한데 l2가 낮다던가)
- Adversarial Loss: super-resolution GAN에서 나옴. Adversarial하게 가장 괜찮은 애만 집어내게 됨
- Perceptual Loss: 사전학습된 로스가 필요하지만, 코드와 학습이 간단하고 데이터 효율적임. Loss network를 고정해서 하는 것
- Style reconstruction loss는 gram matrix를 활용

#### Extension to video

- Flickering problem: 프레임마다 작업하면 연속성이 깨짐
- 전 프레임과 현 프레임을 Conv LSTM을 통해 만들고 temporal loss를 고려하는 연구가 있음. perceptual loss+short term temporal loss+long term temporal loss

</details>

<details>

<summary>6강-Multimodal 1</summary>

<div markdown=”1”>

#### Multimodal이란?

- 이종간의 데이터를 쓸 때 사용되는 용어
- 문제점
    - 데이터 간의 표현식이 다름
    - 특징 공간의 차원 갯수에 밸런스가 안 맞음
    - 모델이 특정 모달리티에 편향될 수 있음
- 그럼에도 불구하고 중요하고 유용함. 매칭, 번역, 참조 등.

#### Data representation

- Visual data
    - 이미지-2,3차원 텐서
    - 비디오-이미지의 스택
    - 3차원 데이터-여러시점이미지, 복셀,분해, 점 구름,메쉬,방정식 등으로 표현
- Text embedding:
    - 직접 사용하기 어려워 토큰 벡터로 변형
    - 임베딩을 밑바닥으로부터 학습시켜야함
    - Word2Vec으로 dense representation을 학습시켰더니 일반화 성능이 좋아짐
- Sound representation
    - 웨이브 폼에서 고속 푸리에 변환을 통해 빈도를 추출하고, 스펙트로그램으로 변환함

#### Multimodal alignment

- 멀티모달 얼라인먼트는 매칭, 번역, 참조 중 매칭에 해당되는 내용. Joint embedding space로 옮겨서 학습함
    - 그 경우 이미지에 태그를 달거나, 반대로 태그로부터 이미지를 생성한다거나 하는 것이 가능
- CLIP(Contrastive Language-Image Pre-training)
    - 이상적인 이미지 인지의 접근: 세상 모든 데이터를 학습하면 분류가 쉬워지지 않을까? 검색만 하면 되니까
    - 4억개 이상의 데이터 페어 수집
    - 이미지 인코더는 ViT-B, 텍스트 인코더는 트랜스포머 사용
    - 도메인에 관계없는 성능을 보여줌
    - **Contrastive Learning**이란? 매칭되는 페어는 양의 점수를, 반대의 경우는 음의 점수를 준다. $N$개의 이미지-텍스트 순서쌍에 대해 $N\times N$개의 코사인 유사도 계산
    - CLIP의 사전 학습법: 대응되는 애들의 점수를 최대화하자.+대응 안되는 애들 점수를 낮추자→대칭 크로스 엔트로피 사용(
    - 활용도: 3년 동안 18000 인용수..말로 표현할 수 없다
- CLIP으로 이미지 자막 넣기
    - ZeroCap(Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic): CLIP과 LLM(GPT-2)를 결합해 추가학습 없이 최적화만으로 자막 찾음
    - GPT-2: 다음 토큰이 마지막 토큰과 과거 키-밸류로 컨디셔닝 됨
    - 제로캡 방법: 다음 단어를 추측해야됨. CLIP 로스가 이미지로부터 모델을 반복적으로 자극해줌. 그러면 크로스 엔트로피 로스가 LLM에서 가장 확률 높은걸 찾아줌. 두 로스의 합→다양한 텍스트 뿐만아니라 OCR 능력까지 발현됨
    - StyleCLIP(Text-driven manipulation of StyleGAN imagery)
    - 매핑 네트워크가 특정 텍스트 명령에 대해 학습됨
    - $L_\text{CLIP}$은 조작된 이미지와 텍스트 명령 사이의 간극을 줄여줌
    - $L_\text{ID}$는 조작된 이미지와 원본 이미지 사이에 변화를 줄여줌(사전 학습된 얼굴 인식 모델의 로스)
    - $L_2$는 원본 이미지의 시각적 특징을 유지해줌(매핑 네트워크의 아웃풋의 $L_2$ 노름)
- CLIP으로 3D 아바타(CLIP-Actor): Text-driven Recommendation and Stylization For Animating Human Meshes
- ImageBIND: One Embedding Space To Bind Them All
    - Joint Embedding을 InfoNCE Loss를 통해 최적화(CLIP에서도 쓰는 로스 아닌가?)

#### Cross-modal translation

- 매칭, 번역, 참조 중 번역에 해당되는 내용
- Text-to-Image: DALL-E2. CLIP과 Diffusion model을 활용
- Sound-to-Image
    - Modality gap도 있고, 관련 없는 소리도 들어감
    - 모듈 접근법과 50가지 이상의 카테고리 셀렉션을 통해 해결
- Speech-to-Face: Speech2Face
- Image-to-Speech: 이미지를 캡션으로, 캡션을 스피치로
- 결국 최종적으론 두 모달리티를 엮는 방법을 학습하고, 생성은 큰 모델한테 맡기는 게 번역이다.

</div>

</details>

### 개인 학습

[CS231A Lecture 7](https://haneol-kijm.github.io/2024/09/03/CS231A-007) 절반만 함

## 23일차

### 데일리 트렌드

- [VisionTS](https://huggingface.co/papers/2408.17253): Vision Masked autoencoder가 time series에 대한 사전 학습 없이 시계열 미래예측을 잘 한다는 터무니 없는 연구 내용; 이미지와 시계열이 사실은 유사하다고? 논문 본문이 궁금해질 정도
- [LLM에게 질문을 2번 읽게 하는 것](https://x.com/rohanpaul_ai/status/1830569747538202874)으로 성능을 크게 향상 시킬 수 있다: 이것도 어이가 없긴 하지만 직관적으로 이해는 되는 부분. 2번 읽으면 여러 부분을 참조하게 되서 성능이 좋아질법 하다.

### 부캠 강의

<details>

<summary>7강-Multimodal 2</summary>

<div markdown=”1”>

#### Visual-language model

- 우리는 일반 인공지능에 가까워지고 있는가? 인간의 지능이란 본래 인지+사고로 이루어지는데, multimodal interface가 인지를 시키고 LLM이 사고를 하기 시작했다.
- 오늘은 매칭, 번역, 참조 중 참조에 해당하는 내용. 여러 모달리티를 거치는 사고과정에 대해 알아보자
- 초창기 시각언어모델
    - Show, attend and tell: 이미지에서 conv feature extraction 이후 RNN으로 집중시킴
    - 트랜스포머: 재귀적이지도 않고 conv layer도 없고 attention만 씀. 장기 의존도가 높아짐.universal함.
    - Flamingo: Visual Language model for few-shot learning
    - 비전 인코더와 Language model을 얼리고, perceiver resampler와 gated cross attention 레이어를 끼워넣어 새로 학습합
    - **perceiver resampler**(VLM 트렌드 논문에도 등장)는 다양한 크기의 시각 피쳐를 고정된 사이즈로 바꿔줌: learned latent query와 어텐션, FFW 활용. tanh gating으로 점점 새로운 내용을 넣어주는게 특이한 포인트.
    - 적은 파라미터로 few-shot learning이 가능해짐

#### LLaVA

- LLaVA: Large Language and Vision Assistant→이미지가 주어졌을 때 이미지에 대한 대화가 가능
- Feature alignment(projection)
    - 사전 학습된 LLM과 비전 인코더 사용
    - projection layer(linear) 단 하나만 추가해서 비전 피쳐를 언어 토큰으로 변환
- Visual Instruction tuning: 학습에 필요한 데이터는 ChatGPT 같은 걸로 생성
- 학습과정
    1. LLM과 비전 인코더를 얼리고 projection layer만 학습
    2. 비전 인코더만 얼리고 언어모델도 같이 학습을 돌려서 fine-tune 해줌

#### InstructBLIP

- 비슷하게 이미지가 주어졌을 때 이미지에 대한 대화가 가능
- LLaVA랑 구조가 비슷하지만, projection layer 자리에 QFormer라는 복잡한 레이어 사용
- QFormer는 Flamingo와 아이디어가 비슷한데,
    - learnable query를 써서 고정사이즈 출력을 내도록 함. Instruction도 같이 넣어줘서 연관된 visual feature를 뽑도록 함
    - fc로 또 바꿔주고 instruction을 또 같이 써서 LLM에 넣어줌
    - QFormer에선 query를 먼저 self-attention으로 instruction과 함께 학습하고, visual feature에 맞추도록 cross attention을 사용
    - 더 잘 학습시키기 위해서 목표를 여러개 잡음. 이미지-텍스트 매칭도 쓰고 이미지 기반 텍스트 생성도 씀. 이를 통해 contrastive learning이 가능
- X-InstructBLIP: 다른 모달리티에도 적용하려는 시도

#### Other visual reasoning

- Visual Programming: 학습 없이 할 수 있는지 시도
    - GPT에게 **문맥이 있는 명령-프로그램 페어**를 주고 프로그램 만들게 시켜서 그 프로그램에 이미지를 넣어서 돌림
    - 문맥이 있는 학습: 예제를 몇가지 주고, 이를 참고해서 답을 내게 유도함. 일종의 few-shot learning
    - 이런 식으로 명령-프로그램 페어 예시를 왕창 줘서 학습시키고, 이걸 기능으로 추가해서 필요한 프로그램을 그때그때 꺼내쓸 수 있도록 함
- PaLM-E: 기존 VLM과 비슷하긴 한데, 생성된 언어로 만든 액션을 control foundation model에 넣어 로봇 제어
- 앞으로 로봇 기술에 많이 적용될 것으로 예상

</div>

</details>

<details>

<summary>8강-Generative model</summary>

<div markdown=”1”>

#### Generative model

- 생성 모델이란? 모델의 확률 분포가 데이터의 확률 분포에 가까워지게 하는 것
- 생성 모델을 쓰는 이유
    - 현실적인 샘플 생성
    - 시계열에 사용→시뮬레이션, 계획 등에 사용됨
    - 데이터 증강
    - 표현 학습
    - 학습된 로스로 사용(이상탐지, 퍼셉츄얼 로스 등): 샘플이 타겟 분포에 속할 확률
    - 조건부 생성 모델: one-to-many, many-to-many 매핑을 모델링
- 생성 모델의 분류
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/f2b64d0d-bdfc-4f3d-afa0-020e2f44ac90/image.png)
    

#### Various generative models

- 확률론 기초
    - Chain rule: $p(x_1, \cdots, x_n) = p(x_1)p(x_2\|x_1)\cdots p(x_n\|x_1, \cdots, x_{n-1})$
    - Bayes’ rule: $p(x\|y)=\frac{p(x,y)}{p(y)}=\frac{p(y\|x)p(x)}{p(y)}$
    - Conditional independence: If $x\mathpallete y\|z$, then $p(x\|y,z)=p(x\|z)$
    - Markov Assumption(1st order): $p(x_n\|x_1, \cdots, x_{n-1})=p(x_n\|x_{n-1})$

##### Autoregressive model

- Explicit density model: 각 픽셀의 조건부 확률로 이미지의 확률을 정하고, 이미지 확률을 최대가 되도록 한다. 근데 너무 복잡하니까 Neural Net으로
    - PixelRNN: 픽셀 간 조건부 확률의 의존성을 RNN으로 모델링

##### VAE(Variational Autoencoer)

- AE(Autoencoder): 이미지를 저차원 표현 공간에 압축시켰다가 다시 원래대로 복구. 저차원이어야되는 이유는 차원이 같으면 Identity map이 되는 오버피팅이 발생해버림
- AE는 memerization(overfitting) 문제, 근처 임베딩이 의미론 적으로 의미가 없는 문제, 확률적 해석이 불가능한 문제, 수학적으로 새로운 샘플을 만들지 못하는 문제가 있다.
- VAE는 AE와 구조는 비슷하나, AE는 고정된 벡터로 투영시키지만 VAE는 인풋을 확률 분포로 넣는다는 차이점이 있다.
- $\mathbf{z}$를 probabilistic encoder $q_\phi(\mathbf{z}\|x)$로 모델링하고, sampled latent vector $\mathbf{z}$에 대해 probabilistic decoder $p_\theta (x\|\mathbf{z})$로 모델링하여 $x$를 생성한다.
- Variational lower bound를 통해 계산

##### DDPM(Denoising Diffusion Probablistic Model)

- 가우시안 노이즈를 점차 더해주는 연산을 만들고, 이를 역으로 돌려서 생성하는 방식. 마르코브 프로세스의 정방향 역방향을 사용한다.
- 마르코브 forward: $q(x_t\|x_{t-1}):=\mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)$ where $\{\beta_t\in (0,1)\}^T_{t=1}$ is a variance schedule
- $q(x_{t-1}\|x_{t})$가 추정이 어렵기 때문에, 모델이 대신 $p_\theta(x_{t-1}\|x_{t})$를 학습하게 함. 정확히는 $p_\theta(x_{t-1}\|x_{t}):=\mathcal{N} (x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)$에서 $\mu_\theta(x_t, t)$를 학습하고자 함. 분산은 학습안됨.
- VAE처럼 손실함수를 잘 변형하여 VLB로 계산할 수 있음
- DDPM의 문제점:
    - 픽셀 공간에 직접 작용
    - 최적화가 GPU 수백일 잡아먹음
    - 수열을 따라 샘플링을 하니 너무 비용이 비쌈

##### Latent Diffusion(Stable Diffusion)

- 사전학습된 강력한 autoencoder의 latent space에 diffusion을 적용
- autoencoder의 인코더, 디코더를 얼려놓고 latent space(예를 들면 64x64 공간)에서 diffusion 학습
- Stable diffusion: cross attention layer를 써서 conditional input을 받음
- 각 모델의 손실함수 차이점:
    - diffusion model: 그냥 이미지+시간에 로스 때림
    - latent diffusion: latent variable+시간에 로스 때림
    - stable diffusion: latent variable+시간+domain specific encoder를 거친 conditioning input에 로스 때림

##### ControlNet

- Conditioned Stable Diffusion with learned conditions
- 캐니 엣지나 휴먼 포즈 같은 컨디션 추가 가능
- 컨트롤넷의 구조
    - 일단 네트워크 하나를 학습시킴
    - 그 네트워크를 얼리고, 파라미터를 그대로 베끼고, 인풋을 원래 인풋+컨디셔닝 벡터의 제로컨볼루션으로 받아서 원래 네트워크 아웃풋에 더하도록 추가 학습
- 다중 컨디션도 적용 가능, 텍스트 명령 없이 이미지 통제 가능

##### LoRA(Low-Rank Adaptation)

- 파라미터를 과하게 쓴 모델들은 사실 저차원에 존재하는 게 아닐까?
- 가정: 모델 adaptation으로 weights를 바꿔도 저차원에 존재할 것이다.
- 원리: 랭크 분해 행렬의 최적화
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/e5ba8ae7-5cea-4bca-a5d0-bf27a1c90d1d/image.png)
    
- 구체적으로 논문 읽어봄:
$h=W_0x$인 걸 $h=W_0x+\delta Wx$로 바꾸고, $\delta W=BA$인데 $ B\in\mathbb{R}^{d\times r}, A\in\mathbb{R}^{r\times k}, r<<\min(d,k)$임. $A$는 가우시안, $B$는 0으로 초기화해서 걍 학습하면 끝
- 파라미터를 1만배, gpu 메모리를 3배 줄일 수 있다.
- few-shot input으로 개인 최적화가 가능

#### Applications of Diffusion models

##### Image Editing

- Prompt-to-Prompt Image editing with Cross Attention Control
    - 텍스트로만 이미지를 통제하면 결과가 심하게 변하는 경우가 잦음
    - 원본 형태를 유지한 채 명령만 바꿔서 이미지를 바꿀 수 있음
    - 보통 이미지의 형태와 모양은 크로스 어텐션에 영향을 많이 받더라→ 따라서 각 텍스트 토큰에 대해 공간 어텐션 맵이 존재함. 그럼 알고리즘에서 diffusion 2개를 돌릴 때, 원본 어텐션맵에 원하는 프롬프트의 어텐션 맵의 차이를 추가해줘서 갈아끼움
    - 다른 editing에 대해서도 적당한 attention map의 조작을 통해 원하는 결과로 유도가 가능함
    - 한계: full description이 필요하고, 단어 하나를 특정해서 조작할 때만 됨
- InstructPix2Pix
    - 그냥 아무 이미지, 아무 명령 1줄 씩 받아오면 수행해줌
    - 원리: GPT3로 캡션을 이쁘게 깎고, 이 캡션으로 stable diffusion으로 이미지를 생성하고, prompt2prompt로 변형된 이미지 페어까지 만듦. 이걸로 그냥 이미지 수정 diffusion model을 fine-tuning+지도학습

##### Depth generation

- Marigold: monocular depth estimation을 diffusion model과 finetuning protocol로 해결
- 원리: 사전학습된 latent diffusion model 이미지 생성기를 이미지 조건부 depth estimator로 바꾼다
    - 사진과 depth를 그냥 3차원 이미지로 보고 둘 다 각각 latent encoder에 넣는다.
    - encoded depth latent에 노이즈를 넣어준다.
    - 이걸 이미지 latent와 concat하고 latent diffusion U-Net이 이걸로 objective를 계산하게 시킨다.
- 추정할 때는 depth는 그냥 가우시안 noise로 넣어줘서 latent 이미지와 합치고  denoise하게 한다. 그 이후 latent decoder 사용
- 제로샷으로 예측이 가능

</div>

</details>

<details>

<summary>접기/펼치기 9강-3D Understanding</summary>

<div markdown=”1”>

#### 3차원이 중요한 이유

- 우리가 사는 세계가 3차원
- 응용: AR, VR, Robot, 3D printing, Medical application

#### 3차원과 2차원의 관계, SFM

- 이미지는 3차원을 2차원에 투영시킨 것, 카메라는 그걸 해주는 도구
- SFM: 두 시점으로부터 움직임을 복구
- 대응되는 이미지 위의 두 점 페어로부터, 카메라 행렬과 3차원 점의 위치 추정
- 카메라 행렬: 3차원 점을 2차원 점에 투영시키는 행렬
- SFM pipeline(예: COLMAP): 이미지 집합→특징 추출, 매칭→(2 view SfM)초기화 단계→(>3 view SfM)성장 단계
- 3D 재건축: incremental RGB-D 스캔 등

#### 3D 데이터 표현

- 3차원 데이터 표현은 이미지처럼 유일하지 않고 다양함
- 텍스쳐 메쉬(ShapeNet): 점, 면, 텍스쳐로 구성
    - 텍스쳐: 2차원-3차원 대응 맵+2차원 텍스쳐 맵
    - Scene-level mesh: object+background
- LiDAR scan으로 3차원 점구름(자율 주행에서 잘 쓰임)

#### 3D 작업-인지

- 3d object recognition: Volumetric CNN 등으로 2차원처럼 내용물 맞춤
- 3D object detection: 이미지나 3D 공간에서 3D 물체 위치 추정, 자율주행
- 3D semantic segmentation: neuroimaging 등에서 많이 쓰이는 3차원 semantic seg.

#### 3D 작업-복구

##### NeRF

- 한정된 2차원 시각 몇 개만으로 새로운 2차원 시각을 합성하는 것
- spatial location, viewing direction을 Fully connected network를 통해 output color와 output density 추출
- Volumetric rendering: 3D 볼륨 데이터로 2차원 이미지를 계산하는 것.
    - 3차원 모델에서 나오는 광선들의 모음이 이미지 픽셀에 모인 량을 적분.
    - 적분할 대상은 Volume density/occupancy*color*accumulated transmittance
    - 멀리서 온 광선의 영향을 줄이기 위해 accumulated transmittance란 함수 곱해줌
    - 실제 계산은 힘드므로 sampling 기반의 계산식으로 대체
- 학습: 광선을 쏨→ 광선을 픽셀로 렌더링→backprop으로 복구 에러를 최소화
- Positional encoding: 저차원 인풋을 고차원으로 올리기 위해 고주파 함수 사용. 고주파 변동이 많은 데이터에 유용
- View dependent color(논-람버셜 효과): 시각에 따라 다른 색깔을 만들 수 있음
- 단점: scene이 여러 개 필요함. overfitting이 필요, 학습이 매우 느리고 렌더링 매우 느림

##### 3D Gaussian Splatting(3DGS)

- 10배 빠른 방법, FPS도 훨씬 높음
- 3D 공간에서 국부적인 부분을 하나의 3D gaussian으로 모델링. 점구름보다 훨 적은 파라미터 요구
- Mean, covariance, opacity, color parameter(view independent), spherical harmonics coeff.(view dependent)
    - 색마다 3개의 spherical harmonics basis 사용(sphere 위의 Fourier basis)
- Covariance 주의사항
    - covariance를 positive semi-definite이 되도록 학습시키기는 어려움
    - 따라서 covariance를 rotation(4차원 quaternion 사용)과 3DoF scaling matrix의 곱으로 표현함: $\Sigma=RSS^\top R^\top$
- 학습
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/9cc390b8-f993-4a6a-ad8c-3dc10d3000b4/image.png)
    
    - Projection에서 카메라 행렬 곱해서 사용.
    - NeRF는 광선 위의 점들을 수집하지만 얘는 3D Gaussian splatting 공간들을 모아버림. 그래서 한방에 이미지를 렌더링해버림
    - rasterization으로 대충 픽셀화해서 ray tracing보다 퀄은 떨어지지만 빠름
    - density control module이 3D gaussian을 조금씩 바꿔줌

#### 3D 작업-생성

- Mesh R-CNN=Mask R-CNN에 3차원 표면 회귀 브랜치 추가

##### DreamFusion

- text-to-3D를 기존 사전학습된 text-to-image 모델 써서 해결
- 학습없이 각 샘플별로 최적화하는 방식. 제로샷으로 가능
    - 3D 표현식(e.g. NeRF)
    - 미분가능한 렌더링: 3차원에서 2차원
    - Score Distillation Sampling(SDS)
    - Backprop(2D to 3D)
- Score Distillation Sampling(SDS) Loss
    - Diffusion model의 noise prediction으로 업데이트
    - 단 diffusion model은 얼림. U-Net 자코비안 계산 비용이 비싸서. 이걸 생략함으로써 효과적인 그라디언트를 얻음

##### Paint-it: Text-to-Text Synthesis by SDS loss

- text-to-texture
- SDS는 유용하지만 loss가 noisy해서 이걸 해결하려는 연구가 많다
- Paint-it에서는 SDS는 그대로 쓰고, texture 맵을 Deep Convolutional PBR Texture Map(DC-PBR)로 학습시켜 해결함. 이 네트워크가 regularization 효과를 낳고 SDS noise를 필터링해줌.
- 수학적으로 해결못하는 SDS를 그냥 다른 부분에서 해결할 수 있다.

</div>

</details>

### 개인 학습

[CS231A Lecture 7 SFM](https://haneol-kijm.github.io/2024/09/03/CS231A-007) 공부 및 정리 마무리

## 24일차

### 데일리 트렌드

- [OLMoE](https://huggingface.co/papers/2409.02060): Mixture of Experts 방법을 쓴다는데 무슨 방법인지 소개가 안나와있어서 논문을 직접 읽어봤다. 제미니와 GPT4에 적용된 비용-성능 반비례 문제를 개선하는 기술이며, 오픈소스로 열려있는 게 적어서 이걸 오픈 소스로 공개한 데에 큰 의의가 있는 것으로 보인다.
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ac84168d-557f-4919-b37f-2632c6456077/4261175d-e372-42ac-a52c-15a862d97910/image.png)
    
    Dense LM의 FFN을 MoE module로 대체하는데, 이 모듈에선 $N_E$개의 FFN(experts)가 존재하여 루터가 인풋을 k개의 experts로 보낸다고 한다.
    
- [LongRecipe](https://huggingface.co/papers/2409.00509): LLM의 long context 한계를 극복하기 위해 Impactful Token Analysis, Position Index Transformation 등을 도입하여 완전 전체를 보는 것보다 85% 리소스 절감을 할 수 있다고 한다.
- [MarioVGG](https://x.com/_akhaliq/status/1831350700846309457): Video Game Generation: A Practical Study using Mario: 그냥 영상이 신기함

### 부캠 강의

<details>

<summary>접기/펼치기 10강-3D Human</summary>

<div markdown=”1”>

#### 인간 모델

- 활용도: 자율주행, 로봇, VR, AR, 인간 아바타 제작
- 크게 인간 모델 인지 분석과, 인간 모델 생성(+옷입히기)로 나뉨
- 어려운 이유: 2D 공간에서 잃어버린 3D 정보, 비정상적인 포즈(고차원), 낮은 대비, 자기생략, 배경, 큰 변동, 번개, 생략, 몸매가 다름, 옷 등

####신체 모델

- 신체모델 모델링의 목표:
    - 단순한 수학적 모델로 신체 형태를 모델링.
    - 실제 사람처럼 생기고 움직여야함.
    - 저차원, 미분가능, 관절이 있고 쉽게 영상화하고 데이터에 맞출 수 있어야함
    - 일반적인 그래픽 툴로 쓸 수 있어야함
- 과정: 4차원 스캐너로 3차원 신체를 60fps로 수천명 측정→포즈, 모양, 역학, 텍스쳐를 받아 **신체 모델 $M$**이 3차원 메쉬로 반환
- 출력값: 각 파트별로 고정되는 3차원 메쉬
- SMPL: 7천개의 3차원 점으로 표현. 즉 21000개의 숫자로 신체를 표현
- 문제는 이 숫자가 물리적인 연관성이 없음→더 저차원에서 통제가능한 모델을 만들자
- 분해된 모델: 템플릿 모델로부터 매개변수를 써서 변화한 모델. 학습이나 추정이 쉬워짐

##### SMPL 매개변수: 

- $M(\theta, \beta; \mathbf{T}, \mathcal{S},\mathcal{P},\mathcal{W}, \mathcal{J})$
    - $\mathbf{T}$ Template(평균 형태)
    - $\mathcal{S}$ Shape blend shape matrix
    - $\mathcal{P}$ Pose blend shape matrix
    - $\mathcal{W}$ Blend weights matrix
    - $\mathcal{J}$ Joint regressor matrix
- 모양 학습 데이터: 성별당 2000 메쉬, 템플릿 메쉬와 대응됨, 같은 포즈로 노말라이즈
- Shape blend shape matrix
    - 인간의 형태는 가우시안 분포로 잘 표현 가능
    - 21000개의 고차원 벡터로 표현되지만 너무 높음. 비선형 딥러닝 쓰기엔 데이터 수가 적음
    - 벡터로 표현해서 평균 내주고 normalize 한 뒤, 각 사람을 행렬의 열로 만들어 PCA로 저차원(10~300)에서 다룸($\mathcal{S}(\beta)$)
- Standard Skinning(Pose blend shape matrix, Blend weights matrix)
    - 쉬는 포즈 버텍스 $\mathbf{T}$, 관절 위치 $\mathbf{J}$, weights $\mathcal{W}$(관절이 각 버텍스에 미치는 영향), 포즈 파라미터 $\theta$로 구성
    - 포즈 파라미터는 쉬는 포즈 버텍스의 관절기준 회전을 표현하므로 관절과 차원 크기가 같음
    - 스키닝 함수 $W(\mathbf{T}, \mathbf{J}, \mathcal{W}, \theta)$는 이들을 받아 버텍스로 보냄

##### Linear Blend Skinning(LBS)

- 가장 일반적이고 단순한 스키닝 모델. 버텍스를 변형된 템플릿 버텍스의 선형 결합 취급
- 블렌딩이 없다면 포즈 변형에서 아티팩트가 발생함
- 픽셀 별 변형과 블렌딩 웨이트를 통해 쉬는 버텍스를 옮겨줌: $\bar{\mathbf{t}}’_i = \sum^K_{k=1}w_{k,i}G’_k(\vec{\theta},\mathbf{J})\bar{\mathbf{t}}_i$
- 문제: 캔디 래퍼, 콜랩스 등의 문제가 존재
- 해결책: Pose Blend Shape로 보정

##### SMPL

- Joint Regressor Matrix: 템플릿과 곱해줘서 조인트로 변환
- 최종적으로 additive model로 구현
- 학습은 surface reconstruction error 최소화로 진행
- 이렇게 학습된 데이터는 3차원 모델링에 많이 쓰이나, 얼굴이나 손을 못 구현하는 문제가 있음
- FLAME face model: SMPL과 비슷한 방식
- MANO hand model

#### SMPL을 이미지로부터

- 2D image를 SMPL로 구현하는 건 쉽지 않다(depth 부족)

##### SMPLify

- 2차원 이미지에서 관절 추출(Bottom-up), 2d point estimation
- 3차원 SMPL 모델을 카메라 행렬로 2차원으로 바꿈(Top-down)
- 그 이후 두 이미지 사이 에러 계산
- 문제점:
    - depth ambiguity→pose and shape prior
    - 관통 문제: 표면을 캡슐화, 개별화 교차 등
- prior term
    - joint prior: 관절이 올바르게 접히는지 고려
    - pose prior: 포즈 데이터셋으로 가우시안 믹스쳐 모델을 추가
    - shape prior: PCA에서 구해진 값들로 gaussian likelihood
    - interpenetration prior: 캡슐 근사의 교집합이 최소화되도록

##### SPIN: SMPL oPtimization IN the loop

- 뉴럴네트워크 활용

##### MultiPly: Reconstruction of Multiple People from Monocular Video in the wild

</div>

</details>

### 개인 학습

[CS231A Lecture 8 정리](https://haneol-kijm.github.io/2024/09/05/CS231A-008)

## 25일차

### 데일리 트렌드

[LinFusion](https://huggingface.co/papers/2409.02097): 현대 diffusion model의 속도와 비용 문제를 linear하게 줄여보겠다는 내용. 내용도 좋지만 mamba, mamba 2 등 기존 모델에 대해 survey하는 느낌도 있어서 주말에 읽어보고 싶다.

[LongLLaVA](https://huggingface.co/papers/2409.02889): 또다른 long-context LLM. 긴 문맥이 요즘 핫한 이슈인지 계속 보인다.

[Loopy](https://x.com/_akhaliq/status/1831530803635085766): Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency. 기존 오디오-to-비디오 모델 들은 모션 템플릿을 종종 썼는데, 얘는 오디오만으로 만들 수 있다고 한다.

[제미니 플래시가 gpt4랑 동점](https://x.com/zacharynado/status/1831792639412465854)을 냈다는 소식: lightweight model이 이리 강할 수 있나??

### 부캠 강의

퀴즈 10: bipartite matching 직접 구현해보니까 번거롭기는 하지만 이해는 확실히 됐음..

attention rollout 논문:

### 개인 학습

CS231A Lectuer 8 course note까지 정리 완료

이력서

## 5주차 후기

- 좋았던 점:
    - 부트캠프 강좌+개인 공부(디퓨전)+CS231 스터디 3종으로 1주일을 빡빡하게 사용했다.
    - 특히 스터디는 하루 3시간 씩 투자하여 github와 github page 기록을 알차게 채울 수 있었다.
    - Diffusion 공부를 이번 주에 거의 마무리하게 되어 기분이 좋다.
    - 이번 주 아침 일찍 일어나 운동 꾸준히 하기에 3일 성공했다.
- 아쉬운 점:
    - **이력서 다듬기**는 다음주부터 Diffusion 공부 시간이 빠지므로 개인 학습 시간을 활용해서 진행하고 싶다.
    - 오늘 비가 온다고 의지가 약해져서 운동을 안 갔음
- 도전할 점: 시간 관계상 주말을 활용해서 추가로 진행하고 싶다
    - 1주일에 코테 문제 1개씩 풀기
    - 주말 이력서 마저 써보기
    - 논문 1편씩 주말에 읽기
- 알게된 점:
    - 부캠 수업을 통해 Diffusion model과 ViT 사이의 모델들에 대한 공부나 개념이 부족한 걸 알게 되었다. 추가 공부가 필요하지 않을까 싶다.
